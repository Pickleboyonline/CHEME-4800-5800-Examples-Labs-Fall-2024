





include("Include.jl");


function rbf(x::Tuple{Int,Int},y::Tuple{Int,Int}; σ = 1.0)::Float64
    d = sqrt((x[1] - y[1])^2 + (x[2] - y[2])^2);
    return exp(-d/(2*σ^2))
end;





number_of_rows = 10
number_of_cols = 10
nactions = 4;
γ = 0.95;
number_of_random_steps = 10;
nstates = (number_of_rows*number_of_cols);
𝒮 = range(1,stop=nstates,step=1) |> collect;
𝒜 = range(1,stop=nactions,step=1) |> collect;





# setup rewards -
lava_reward = -1000.0;
charging_reward = 100.0;
softwall_reward = -2000.0;
σ = 1.0;
default_reward = -0.1;
charging_station_coordinates = (3,4);

rewards = Dict{Tuple{Int,Int}, Float64}()
rewards[(2,2)] = lava_reward # lava in the (2,2) square 
rewards[(4,4)] = lava_reward # lava in the (4,4) square
rewards[charging_station_coordinates] = charging_reward    # charging station square

# walls?
soft_wall_set = Set{Tuple{Int,Int}}(); # none for now ...

# setup set of absorbing states -
absorbing_state_set = Set{Tuple{Int,Int}}()
push!(absorbing_state_set, (2,2));
push!(absorbing_state_set, charging_station_coordinates);
push!(absorbing_state_set, (4,4));





# do some shaping?
is_reward_shaping_on = true;
if (is_reward_shaping_on == true)
    for s in 𝒮
        for s′ in 𝒮
            coordinate = (s,s′);
            if (haskey(rewards, coordinate) == false && in(coordinate,soft_wall_set) == false && 
                    in(coordinate,absorbing_state_set) == false)
                rewards[coordinate] = default_reward + charging_reward*rbf(coordinate, charging_station_coordinates, σ = σ);
            end
        end
    end
end


rewards





world = VLDecisionsPackage.build(MyRectangularGridWorldModel, 
    (nrows = number_of_rows, ncols = number_of_cols, rewards = rewards));


world.moves





R = zeros(nstates, nactions);
fill!(R, 0.0)
for s ∈ 𝒮
    for a ∈ 𝒜
        
        Δ = world.moves[a];
        current_position = world.coordinates[s]
        new_position =  current_position .+ Δ
        if (haskey(world.states, new_position) == true)
            if (haskey(rewards, new_position) == true)
                R[s,a] = rewards[new_position];
            else
                R[s,a] = -1.0;
            end
        else
            R[s,a] = -50000.0; # we are off the grid, big negative penalty
        end
    end
end
R





T = Array{Float64,3}(undef, nstates, nstates, nactions);
fill!(T, 0.0)
for a ∈ 𝒜
    
    Δ = world.moves[a];
    
    for s ∈ 𝒮
        current_position = world.coordinates[s]
        new_position =  current_position .+ Δ
        if (haskey(world.states, new_position) == true && 
                in(current_position, absorbing_state_set) == false)
            s′ = world.states[new_position];
            T[s, s′,  a] = 1.0
        else
            T[s, s,  a] = 1.0
        end
    end
end





m = VLDecisionsPackage.build(MyMDPProblemModel, 
    (𝒮 = 𝒮, 𝒜 = 𝒜, T = T, R = R, γ = γ));


m.𝒮





d = number_of_random_steps;





function myrandpolicy(problem::MyMDPProblemModel, 
        world::MyRectangularGridWorldModel, s::Int)::Int
    
    # initialize -
    d = Categorical([0.25,0.25,0.25,0.25]); # you specify this LRDU
    
    # should keep choosing -
    should_choose_again = true;
    a = -1; # default
    while (should_choose_again == true)
       
        # initialize a random categorical distribution over actions -
        aᵢ = rand(d);
        
        # get the move and the current location -
        Δ = world.moves[aᵢ];
        current_position = world.coordinates[s]
        new_position =  current_position .+ Δ
        if (haskey(world.states, new_position) == true)
            a = aᵢ
            should_choose_again = false;
        end
    end
    
    return a;
end;


function myrandstep(problem::MyMDPProblemModel, 
        world::MyRectangularGridWorldModel, s::Int, a::Int)
    
    r = problem.R[s,a]; # get the reward value for being in state s, and taking action a
    Δ = world.moves[a]; # this action does this move    
    current_position = world.coordinates[s]; # get where we are now
    new_position =  current_position .+ Δ; # propose a new position
    
    s′ = s; # default, we don't do anything, stay where you are
    if (haskey(world.states, new_position) == true)
        s′ = world.states[new_position];
    end
    
    # return -
    return (s′,r) # This returns the next state and the reward at the current position
end;


function myrollout(problem::MyMDPProblemModel, 
        world::MyRectangularGridWorldModel, s::Int64, depth::Int64)::Float64
    
    # initialize -
    ret = 0.0;
    for i ∈ 1:depth
        a = myrandpolicy(problem, world, s);
        s, r = myrandstep(problem, world, s, a);
        ret += problem.γ^(i-1)*r;
    end
    return ret;
end;





U(s) = myrollout(m,world,s,d)





utility_array = Array{Float64,1}();
for s ∈ 𝒮
    push!(utility_array, U(s))
end


utility_array





my_Q = Q(m, utility_array)





my_π = policy(my_Q);


my_π





move_arrows = Dict{Int,Any}();
move_arrows[1] = "←"
move_arrows[2] = "→"
move_arrows[3] = "↓"
move_arrows[4] = "↑"
move_arrows[5] = "∅";


for s ∈ 𝒮
    a = my_π[s];
    Δ = world.moves[a];
    current_position = world.coordinates[s]
    new_position =  current_position .+ Δ
    
    if (in(current_position, absorbing_state_set) == true)
        println("$(current_position) $(move_arrows[5])")
    else
        println("$(current_position) $(move_arrows[a]) $(new_position)")
    end
end


let

    # setup 
    world_model = world;
    startstate = (1,1)

    # draw the path -
    p = plot();
    initial_site = startstate
    hit_absorbing_state = false
    s = world_model.states[initial_site];
    visited_sites = Set{Tuple{Int,Int}}();
    push!(visited_sites, initial_site);

    s′ = s;
    while (hit_absorbing_state == false)
        
        current_position = world_model.coordinates[s′]
        a = my_π[s′];
        Δ = world_model.moves[a];
        new_position =  current_position .+ Δ
        scatter!([current_position[1]],[current_position[2]], label="", showaxis=:false, msc=:black, c=:blue)
        plot!([current_position[1], new_position[1]],[current_position[2],new_position[2]], label="", arrow=true, lw=1, c=:gray)

        s′ = nothing;
        if (in(new_position, absorbing_state_set) == true || in(new_position, visited_sites) == true)
            hit_absorbing_state = true;
        elseif (haskey(world_model.states, new_position) == true)
            s′ = world_model.states[new_position];
            push!(visited_sites, new_position);
        else
            hit_absorbing_state = true; 
        end
    end

    # draw the grid -
    for s ∈ 𝒮
        current_position = world_model.coordinates[s]
        a = my_π[s];
        Δ = world_model.moves[a];
        new_position =  current_position .+ Δ
        
         if (haskey(rewards, current_position) == true && rewards[current_position] == charging_reward)
            scatter!([current_position[1]],[current_position[2]], label="", showaxis=:false, c=:green, ms=4)
        elseif (haskey(rewards, current_position) == true && rewards[current_position] == lava_reward)
            scatter!([current_position[1]],[current_position[2]], label="", showaxis=:false, c=:red, ms=4)
        elseif (in(current_position, soft_wall_set) == true)
            scatter!([current_position[1]],[current_position[2]], label="", showaxis=:false, c=:gray69, ms=4)
        else
            if (is_reward_shaping_on == true)
                new_color = weighted_color_mean(rbf(current_position, charging_station_coordinates, σ = σ), colorant"green", colorant"white")
                scatter!([current_position[1]],[current_position[2]], label="", showaxis=:false, msc=:lightgray, c=new_color)
            else
                scatter!([current_position[1]],[current_position[2]], label="", showaxis=:false, msc=:black, c=:white)
            end
        end
    end
    current()
end



