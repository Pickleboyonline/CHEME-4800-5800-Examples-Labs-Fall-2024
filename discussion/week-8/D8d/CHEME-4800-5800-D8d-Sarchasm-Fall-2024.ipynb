{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a60d01-30b1-4b36-8386-58c9dc2b30d5",
   "metadata": {},
   "source": [
    "# Lab 8d: Building a Linear Regression Sarchasm Classifier\n",
    "In this lab, students will create a two-class linear classifier to predict whether a news headline is sarcastic or not. We'll estimate the parameters of this classifier using the ordinary least squares with and without regularization.  \n",
    "\n",
    "### Learning objectives and tasks\n",
    "* __Prerequisites__: To save some time, we'll load the saved file from the `SarcasmSamplesTokenizer` example from `week-4` using [the `load(...)` method exported by the FileIO.jl package](https://github.com/JuliaIO/FileIO.jl). We'll then use this data in the subsequent calculations.\n",
    "* __Task 1__: Compute the expected value of the parameters $\\beta$ without regularization. In this task, we estimate the parameters of a linear regression model that maps the text token sequences to the headline labels without regularization.\n",
    "* __Task 2__: Compute the expected value of the parameters $\\beta$ with regularization. In this task, we'll explore the effect of regularization on our ability to classify news headlines as sarcastic or not sarcastic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be8704d-2ac8-412c-9692-c2936715dfb1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We set up the computational environment by including [the `Include. jl` file](Include.jl) using [the `include(...)` method](https://docs.julialang.org/en/v1/base/base/#Base.include). The [`Include.jl` file](Include.jl) loads external packages and functions we will use in these examples. \n",
    "* For additional information on functions and types used in this example, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20aa1055-5a47-4b32-a5b8-c170ca2eaeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb17c7f-8129-4a58-a66f-a2eecf80889d",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "To save some time, we'll load the saved file from the `SarcasmSamplesTokenizer` example using [the `load(...)` method exported by the FileIO.jl package](https://github.com/JuliaIO/FileIO.jl). To load the `jld2` (binary) saved file, we pass the path to the file we want to load the [`load(...)` function](https://github.com/JuliaIO/FileIO.jl). This call returns the data as a [Julia `Dict` type](https://docs.julialang.org/en/v1/base/collections/#Base.Dict). \n",
    "* Let's set the path to the save file in the `path_to_save_file::String` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3a524a-28e0-4870-a682-ba4ea4a66c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_file = joinpath(_PATH_TO_DATA, \"L4a-SarcasmSamplesTokenizer-SavedData.jld2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543b873c-e9f6-46b8-b2b1-7d83386a59d9",
   "metadata": {},
   "source": [
    "Then we load the `jld2` file using [the `load(...)` method](https://juliaio.github.io/FileIO.jl/stable/reference/#FileIO.load), where the contents of the file are stored in the `saved_data_dictionary::Dict{String, Any}` variable. \n",
    "* We saved the `corpusmodel::MySarcasmRecordCorpusModel` instance, which holds the other interesting data, e.g., the `tokendictionary.` Thus, we can get (most) of everything we need from the `corpusmodel.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b4e7e23-19d8-413d-b6a4-52356eadb58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data_dictionary = load(path_to_save_file);\n",
    "\n",
    "# pull data from the saved_data_dictionary -\n",
    "corpusmodel = saved_data_dictionary[\"corpus\"];\n",
    "\n",
    "tokendictionary = corpusmodel.tokens;\n",
    "inversetokendictionary = corpusmodel.inverse;\n",
    "number_of_records = saved_data_dictionary[\"number_of_records\"];\n",
    "\n",
    "# compute some stuff need for later -\n",
    "number_of_tokens = tokendictionary |> length; # size of the token dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762729a1-f60a-46ee-9767-1230a8f32f3b",
   "metadata": {},
   "source": [
    "### Compute the maximum pad length\n",
    "Not every headline has the same length, but we want the token vectors to have the same size. Thus, we'll find the longest vectors in the dataset and pad the token vectors to that length. To do that, let's iterate through each headline, compute its size, and then save this length if it is longer than we've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09477d3f-d9f3-43db-aae2-b2b9b2ef8a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pad_length = 0; # initialize: we have 0 length\n",
    "for i ∈ 1:number_of_records\n",
    "    test_record_length = tokenize(corpusmodel.records[i].headline, corpusmodel.tokens) |> length; # tokenize, and calc the number of tokens\n",
    "    if (test_record_length > max_pad_length)\n",
    "        max_pad_length = test_record_length; # we've found a new longest headline!\n",
    "    end\n",
    "end\n",
    "max_pad_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d596823a-72d2-497d-b75c-228b7257523f",
   "metadata": {},
   "source": [
    "### Compute the vector representation of all headline samples\n",
    "Now that we have found the `max_pad_length::Int64`, we can tokenize all records using the `max_pad_length::Int64` value as the `pad` value in [the `tokenize(...)` method](src/Compute.jl). \n",
    "* We'll use `right-padding` and will store the tokenized records for each headline in the `token_record_dictionary::Dict{Int64, Array{Int64,1}}` dictionary, where the keys of this dictionary are the record indexes and the values of the tokenized records (which are of type `Array{Int64,1}.`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9134e13d-d8ee-4a79-a699-7deee0e7e5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151-element Vector{Int64}:\n",
       " 26617\n",
       " 23295\n",
       " 27980\n",
       "  8295\n",
       "  5553\n",
       " 18533\n",
       " 12047\n",
       " 15828\n",
       "   913\n",
       "   913\n",
       "   913\n",
       "   913\n",
       "   913\n",
       "     ⋮\n",
       "   913\n",
       "   913\n",
       "   913\n",
       "   913\n",
       "   913\n",
       "   913\n",
       "   913\n",
       "   913\n",
       "   913\n",
       "   913\n",
       "   913\n",
       "   913"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_record_dictionary = Dict{Int64, Array{Int64,1}}();\n",
    "for i ∈ 1:number_of_records\n",
    "    \n",
    "    v = tokenize(corpusmodel.records[i].headline, corpusmodel.tokens, \n",
    "            pad = max_pad_length); \n",
    "    token_record_dictionary[i] = v;\n",
    "end\n",
    "token_record_dictionary[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4defb923-0612-46a9-b81b-81d69ac6fcdd",
   "metadata": {},
   "source": [
    "### Compute the vector representation of all labels\n",
    "Finally, let's compute the label dictionary. We'll store the labels in the `label_record_dictionary::Dict{Int64, Int64}` dictionary, where the keys of this dictionary are the record indexes, and the values are the labels: `sarcastic = 1`, while `not sarcastic = -1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c519966d-ff21-4954-9a8f-a10ca152a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_record_dictionary = Dict{Int64, Int64}();\n",
    "for i ∈ 1:number_of_records\n",
    "    label = corpusmodel.records[i].issarcastic;\n",
    "    if (label == true)\n",
    "        label_record_dictionary[i] = 1;\n",
    "    else\n",
    "        label_record_dictionary[i] = -1;\n",
    "    end\n",
    "end\n",
    "label_record_dictionary;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e39c7df-d048-4d56-9cf8-66f00277b163",
   "metadata": {},
   "source": [
    "### Partition the data matrix $\\mathbf{X}$ and the output $\\mathbf{Y}$\n",
    "We can now put the token vectors and the labels into a data matrix $\\mathbf{X}$ and an output vector $\\mathbf{Y}$.\n",
    "* The data matrix $\\mathbf{X}$ will be a `number_of_records` $\\times$ `max_pad_length` array holding `Int64` values (the token indexes). On the other hand, the output (label) vector will be a `number_of_records`$\\times$`1` column vector, holding the label for each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc6955dc-5193-4758-97c8-112c80b7a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = let\n",
    "\n",
    "    X = Array{Int64,2}(undef, number_of_records, max_pad_length);\n",
    "    Y = Array{Int64,1}(undef, number_of_records);\n",
    "    \n",
    "    for i ∈ 1:number_of_records\n",
    "        Y[i] = label_record_dictionary[i]; # get the label (output value)\n",
    "\n",
    "        tokens = token_record_dictionary[i];\n",
    "        for j ∈ 1:max_pad_length\n",
    "            X[i,j] = tokens[j];\n",
    "        end\n",
    "    end\n",
    "    X,Y\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ad921-607e-4a3c-b696-709ec8a9fe35",
   "metadata": {},
   "source": [
    "Finally, let's partition the data into a `training` and `testing` set so that we can determine how well the model can predict unseen data, i.e., how well the model `generalizes.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38447f96-f179-4825-b475-293b2b5472bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 0.80\n",
    "(X_train, X_test, y_train, y_test) = partition(X, Y; trainfraction = fraction); # this is a *random* split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd086cef-52f0-443f-89dd-1b83fcc45644",
   "metadata": {},
   "source": [
    "## Task 1: Compute the expected value of the parameters $\\beta$ without regularization\n",
    "In this task, we estimate the parameters of a linear regression model that maps the text token sequences to the headline labels. \n",
    "We know that the `data matrix` $\\mathbf{X}$ is `overdetermined,` i.e., $m>n$ (more equations than unknowns). Thus, we are solving the minimization problem for an unknown parameter estimates $\\hat{\\beta}$:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{\\beta}} = \\arg\\min_{\\mathbf{\\beta}} ||~\\mathbf{y} - \\mathbf{X}\\cdot\\mathbf{\\beta}~||^{2}_{2}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $||\\star||^{2}_{2}$ is the square of the p = 2 vector norm. Then, the value of the unknown parameter vector $\\mathbf{\\beta}$ that minimizes the sum of the squares loss function for an overdetermined system is given by:\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{\\beta}} = \\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{y} - \\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{\\epsilon}\n",
    "\\end{equation*}\n",
    "The matrix $\\mathbf{X}^{T}\\mathbf{X}$ is called the normal matrix, while $\\mathbf{X}^{T}\\mathbf{y}$ is called the moment matrix. The __expectation__ removes the error term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb61ce4-861c-4f19-8dfa-06498b3121dc",
   "metadata": {},
   "source": [
    "### Check: Does the inverse of the normal matrix exist?\n",
    "Before we estimate the model parameters $\\hat{\\beta}$, let's check if the normal matrix $\\mathbf{X}^{T}\\mathbf{X}$ is invertible by calling [the `rank(...)` function exported by LinearAlgebra.jl](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.rank).\n",
    "* If the normal matrix $\\mathbf{X}^{T}\\mathbf{X}$ has full rank, we can continue. However, if it does not, we must consider an alternative approach. Do this test [using the @assert macro](https://docs.julialang.org/en/v1/base/base/#Base.@assert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59ac71fa-1cd8-4603-bf78-2ca819e155f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "AssertionError: rank(normal_matrix) == max_pad_length",
     "output_type": "error",
     "traceback": [
      "AssertionError: rank(normal_matrix) == max_pad_length",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[20]:2"
     ]
    }
   ],
   "source": [
    "normal_matrix = transpose(X)*X;\n",
    "@assert rank(normal_matrix) == max_pad_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e8fd4-6898-4d3a-8f5e-572df6f8b3a7",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "Alternatively, we can compute the expected value of the parameters using  [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition). Let the [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) of the $n\\times{p}$ data matrix $\\mathbf{X}$ be given by:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\mathbf{U}\\cdot\\mathbf{\\Sigma}\\cdot\\mathbf{V}^{T}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\mathbf{U}$ is an orthogonal matrix, $\\mathbf{\\Sigma}$ is a diagonal singular value matrix,\n",
    "and $\\mathbf{V}$ is an orthogonal matrix. Then, the regularized least-squares estimate of the unknown parameter vector $\\mathbf{\\theta}$ is given by:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{\\theta}}_{\\lambda} = \\Bigl[\\left(\\mathbf{\\Sigma}^{T}\\mathbf{\\Sigma}+\\lambda\\mathbf{I}\\right)\\mathbf{V}^{T}\\Bigr]^{-1}\\cdot\\mathbf{\\Sigma^{T}}\\cdot\\mathbf{U}^{T}\\cdot\\mathbf{y}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13c174a5-76b2-442c-8f1a-65e99555fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "β̂ = let\n",
    "    \n",
    "    (U,d,V) = svd(X_train);\n",
    "    Σ = diagm(d);\n",
    "    IM = diagm(ones(max_pad_length)); # we have max_pad_length parameters -\n",
    "\n",
    "    # compute θ̂ -\n",
    "    M = (transpose(Σ)*Σ)*transpose(V);\n",
    "    β̂ = inv(M)*transpose(Σ)*transpose(U)*y_train;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f83bb7e-1e6f-472f-8a24-d3b16499c1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151-element Vector{Float64}:\n",
       " -5.560106434815856e-6\n",
       " -6.0371704317631966e-6\n",
       " -1.0937902426433418e-6\n",
       " -1.7285330950072338e-6\n",
       "  9.898804545293222e-7\n",
       "  1.2146227706635256e-6\n",
       " -2.428556872274477e-6\n",
       " -1.7354398127010959e-6\n",
       " -4.2780050388636935e-6\n",
       " -1.9861752296580188e-6\n",
       " -2.622158153985228e-6\n",
       "  1.9679296468266063e-6\n",
       "  6.288858502536041e-6\n",
       "  ⋮\n",
       " -1.3372930761239743e9\n",
       " -4.26605938164181e9\n",
       "  2.9019561832030244e9\n",
       " -1.7213787695795991e9\n",
       " -3.129596583136175e7\n",
       " -9.878952070559653e8\n",
       "  4.5146078035189104e8\n",
       " -4.799696932635876e9\n",
       " -2.1830751465770736e9\n",
       "  2.7095469942950583e9\n",
       " -4.975883629941494e9\n",
       "  3.434164501223366e9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "β̂"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771a4c5-166e-4a27-a4c6-92341037d802",
   "metadata": {},
   "source": [
    "### Compute the fraction correctly classified\n",
    "The parameters $\\hat{\\beta}$ can now be used to compute what the model says the labels should be, i.e., whether a story is sarcastic or not. Let's calculate the `y_model_train::Array{Float64,1}` array, which holds the model estimated labels for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7576cc9d-2ca1-4240-b8b2-4c3a33a4551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model_train = X_train*β̂;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69296c7-a70d-4cd7-8b37-213c46be0101",
   "metadata": {},
   "source": [
    "Hmmmm. If a bunch of floating point values. Let's propose a mapping between these values and the sarcasm labels:\n",
    "* if $\\hat{y}_{i}\\leq{0}$: the news headline is __not sarcastic__. Otherwise, for $\\hat{y}_{i}>0$ we classify the news headline as sarcastic.\n",
    "\n",
    "Let's implement this idea and compute the fraction of correctly labeled headlines for the training and testing datasets. We store the fraction of correct labels for the training data in the `f_train::Float64` variable and the fraction of correct labels for the testing data in the `f_test::Float64` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56c78156-583e-4d32-8002-ccad14093f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14976415094339623, 0.14223309453084046)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_train, f_test = let\n",
    "\n",
    "    # compute what the model says the labels should be\n",
    "    y_model_train = X_train*β̂;\n",
    "    y_model_test = X_test*β̂;\n",
    "    number_of_training_samples = length(y_model_train);\n",
    "    number_of_testing_samples = length(y_model_test);\n",
    "    f_train = 0.0;\n",
    "    f_test = 0.0;\n",
    "\n",
    "    # -- TRAINING --------------------------------------------- #\n",
    "    N₊ = 0;\n",
    "    for i ∈ 1:number_of_training_samples\n",
    "        \n",
    "        label = 1; # default sarcasm\n",
    "        if (y_model_train[i] ≤ 0)\n",
    "            label = 0;\n",
    "        end\n",
    "        \n",
    "        if (label == y_train[i])\n",
    "           N₊ += 1;\n",
    "        end\n",
    "    end\n",
    "    f_train = N₊/number_of_training_samples;\n",
    "    # --------------------------------------------------------- #\n",
    "\n",
    "    # -- TESTING ---------------------------------------------- #\n",
    "    N₊ = 0;\n",
    "    for i ∈ 1:number_of_testing_samples\n",
    "        \n",
    "        label = 1; # default sarcasm\n",
    "        if (y_model_test[i] ≤ 0)\n",
    "            label = 0;\n",
    "        end\n",
    "        \n",
    "        if (label == y_test[i])\n",
    "           N₊ += 1;\n",
    "        end\n",
    "    end\n",
    "    f_test = N₊/number_of_testing_samples;\n",
    "    # --------------------------------------------------------- #\n",
    "    \n",
    "    # return -\n",
    "    f_train, f_test\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff94e02-f313-402f-8a47-9d3b957e5728",
   "metadata": {},
   "source": [
    "## Task 2: Compute the expected value of the parameters $\\beta$ with regularization\n",
    "In this task, we'll explore the effect of regularization on our ability to classify news headlines as sarcastic or not sarcastic. \n",
    "Let's see what happens when we add a regularization parameter. If we use `ridge` regularization, i.e., we add a $||\\,\\beta\\,||_{2}^{2}$ term to the objective function:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{\\beta}} = \\arg\\min_{\\mathbf{\\beta}} ||~\\mathbf{y} - \\mathbf{X}\\cdot\\mathbf{\\beta}~||^{2}_{2} + \\lambda\\cdot{||\\,\\beta\\,||_{2}^{2}}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "where $\\lambda\\geq{0}$ is called a `regularization` parameter. This problem has an analytical solution of the form:\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{\\beta}} = \\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\cdot\\mathbf{I}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{y} - \\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\cdot\\mathbf{I}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{\\epsilon}\n",
    "\\end{equation*}\n",
    "The __expectation__ removes the error term. Let's set a value for the regularization parameter $\\lambda\\geq{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4545af14-3911-40ae-9e4c-e871f97c2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 100.0; # select a value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e09a2ad-59ee-4c38-ac32-4ce3d4ba7b75",
   "metadata": {},
   "source": [
    "### Check: Does regularization fix the rank issue?\n",
    "Before we estimate the model parameters $\\hat{\\beta}$, let's check if the regularized normal matrix $\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\cdot\\mathbf{I}$ is invertible by calling [the `rank(...)` function exported by LinearAlgebra.jl](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.rank).\n",
    "* If the regularized normal matrix $\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\cdot\\mathbf{I}$ has full rank, we can continue. However, if it does not, we must consider an alternative approach. Do this test [using the @assert macro](https://docs.julialang.org/en/v1/base/base/#Base.@assert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48077e3e-e0ee-4cb1-a573-99171eaef922",
   "metadata": {},
   "outputs": [],
   "source": [
    "IM = diagm(ones(max_pad_length));\n",
    "regularized_normal_matrix = transpose(X_train)*X_train + λ*IM;\n",
    "@assert rank(regularized_normal_matrix) == max_pad_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c5582-4690-4c9a-bbe8-50822e23cee3",
   "metadata": {},
   "source": [
    "Wow! That is cool. Let's compute the expected value of the regression parameters $\\hat{\\beta}$ by inverting the regularized normal matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ea4d752-7c63-4728-abc2-7908b53c12c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "β̂_reg = let\n",
    "    M = transpose(X_train)*X_train + λ*IM;\n",
    "    β̂_reg = inv(M)*transpose(X_train)*y_train\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2ed8fb3-d7ab-4df0-b371-a93e330cb550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151-element Vector{Float64}:\n",
       " -5.93592616986123e-6\n",
       " -5.353346459971599e-6\n",
       " -5.05217221697683e-7\n",
       " -1.496124563166532e-6\n",
       "  8.700359051415233e-7\n",
       "  5.099607899736908e-7\n",
       " -1.2619829702993616e-6\n",
       " -2.0105457554749383e-6\n",
       " -4.409290668348094e-6\n",
       " -2.194810986647522e-6\n",
       " -2.2422691095262753e-6\n",
       "  2.030717171782424e-6\n",
       "  6.206132842742174e-6\n",
       "  ⋮\n",
       "  8.483468587594203e-6\n",
       "  9.153058872127902e-6\n",
       "  8.256795549318286e-6\n",
       "  7.413361259661839e-6\n",
       "  6.349108596135993e-6\n",
       "  5.233346953255753e-6\n",
       "  6.5147984636412055e-6\n",
       "  5.852345743404349e-6\n",
       "  5.725657585814404e-6\n",
       "  5.227943753587794e-6\n",
       "  5.7853367176147685e-6\n",
       "  6.5710384212248896e-6"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "β̂_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c6160-47de-4a3d-bf81-4e68df8d79f4",
   "metadata": {},
   "source": [
    "### Compute the fraction correctly classified for the regularized case\n",
    "The parameters $\\hat{\\beta}$ can now be used to compute what the model says the labels should be, i.e., whether a story is sarcastic or not. Let's calculate the `y_model_train::Array{Float64,1}` array, which holds the model estimated labels for the training data for the regularized parameters.\n",
    "* We store the fraction of correct labels for the training data in the `f_train_reg::Float64` variable and the fraction of correct labels for the testing data in the `f_test_reg::Float64` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a3af22a-786e-45aa-af57-3491d22d496e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.16024633123689727, 0.1493971693167919)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_train_reg, f_test_reg = let\n",
    "\n",
    "    # compute what the model says the labels should be\n",
    "    y_model_train = X_train*β̂_reg;\n",
    "    y_model_test = X_test*β̂_reg;\n",
    "    number_of_training_samples = length(y_model_train);\n",
    "    number_of_testing_samples = length(y_model_test);\n",
    "    f_train = 0.0;\n",
    "    f_test = 0.0;\n",
    "\n",
    "    # -- TRAINING --------------------------------------------- #\n",
    "    N₊ = 0;\n",
    "    for i ∈ 1:number_of_training_samples\n",
    "        \n",
    "        label = 1; # default sarcasm\n",
    "        if (y_model_train[i] ≤ 0)\n",
    "            label = 0;\n",
    "        end\n",
    "        \n",
    "        if (label == y_train[i])\n",
    "           N₊ += 1;\n",
    "        end\n",
    "    end\n",
    "    f_train = N₊/number_of_training_samples;\n",
    "    # --------------------------------------------------------- #\n",
    "\n",
    "    # -- TESTING ---------------------------------------------- #\n",
    "    N₊ = 0;\n",
    "    for i ∈ 1:number_of_testing_samples\n",
    "        \n",
    "        label = 1; # default sarcasm\n",
    "        if (y_model_test[i] ≤ 0)\n",
    "            label = 0;\n",
    "        end\n",
    "        \n",
    "        if (label == y_test[i])\n",
    "           N₊ += 1;\n",
    "        end\n",
    "    end\n",
    "    f_test = N₊/number_of_testing_samples;\n",
    "    # --------------------------------------------------------- #\n",
    "    \n",
    "    # return -\n",
    "    f_train, f_test\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118d83f4-2e29-4b04-83bb-2851bbf3a1b0",
   "metadata": {},
   "source": [
    "## Hmmm. Both of these approaches give lousy performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6243c4-9951-43c9-86b3-d4dedf6cf253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
