{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf979632-79b3-4c3f-919c-424fd5e3de94",
   "metadata": {},
   "source": [
    "# Example: Linear Regression Parameters using Singular Value Decomposition (SVD)\n",
    "This example will familiarize students with using [Singular Value Decomposition]() to estimate the parameters in ordinary least squares and regularized least squares calculations. We'll examine estimating the parameters in single-index models for equity returns.\n",
    "\n",
    "### Single index model of Sharpe\n",
    "A single index model describes the return of a firm’s stock in terms of a firm-specific return and the overall market return. One of the simplest (yet still widely used) single index models was developed by [Sharpe (1963)](https://en.wikipedia.org/wiki/Single-index_model#:~:text=The%20single%2Dindex%20model%20(SIM,used%20in%20the%20finance%20industry.)). Let $R_{i}(t)\\equiv\\left(r_{i}\\left(t\\right) - r_{f}\\right)$ \n",
    "and $R_{m}(t)\\equiv\\left(r_{m}\\left(t\\right)-r_{f}\\right)$ denote the firm-specific and market **excess returns**, where $r_{f}$ denotes the risk-free rate of return.\n",
    "Further, let $\\epsilon_{i}\\left(t\\right)$ denote stationary normally distributed random noise\n",
    "with mean zero and standard deviation $\\sigma_{i}$. Then, the single index model of Sharpe is given by:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "R_{i}\\left(t\\right) = \\alpha_{i}+\\beta_{i}\\cdot{R}_{m}\\left(t\\right)+\\epsilon_{i}\n",
    "\\left(t\\right)\\qquad{t=1,2,\\dots,T}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "where $\\alpha_{i}$ and $\\beta_{i}$ are (unknown) model parameters: \n",
    "* $\\alpha_{i}$ describes the firm-specific return not explained by the market; thus, $\\alpha_{i}$ is the idiosyncratic return of firm $i$.\n",
    "* $\\beta_{i}$ has two interpretations. First, it measures the relationship between the excess return of firm $i$ and the excess return of the market. \n",
    "A large $\\beta_{i}$ suggests that the market returns (or losses) are amplified for firm $i$, while a small $\\beta_{i}$ indicates that the market returns (or losses) are damped for firm $i$. Second, it represents the relative risk of investing in a firm $i$ relative to the overall market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07550441-a826-4b35-998b-51638f613375",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This example requires several external libraries and a function to compute the outer product. Let's download and install these packages and call our `Include.jl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902e433f-99ee-474a-910a-1d2add60bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb7356d-9e06-4013-8c39-6e7e99f84c3a",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "We gathered a daily open-high-low-close `dataset` for each firm in the [S&P500](https://en.wikipedia.org/wiki/S%26P_500) from `01-03-2018` until `03-14-2024`, along with data for a few exchange-traded funds and volatility products during that time. \n",
    "* We load the `orignal_dataset` by calling the `MyMarketDataSet()` function and then remove firms that do not have the maximum number of trading days, i.e., they are missing data. We save the cleaned data in the `dataset` variable.\n",
    "* We'll also grab the list of firms in the `dataset` and sort them alphabetically. We'll store the sorted list of firms in the `my_list_of_tickers` dataset.\n",
    "* Finally, we'll compute the excess return matrix `R` and the excess return for the market index, in this case treated as `SPY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2387321-1570-45fc-bb91-dfd31cd07a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dict{String,DataFrame}();\n",
    "original_dataset = MyMarketDataSet() |> x->x[\"dataset\"];\n",
    "maximum_number_trading_days = original_dataset[\"AAPL\"] |> nrow;\n",
    "for (ticker,data) ∈ original_dataset\n",
    "    if (nrow(data) == maximum_number_trading_days)\n",
    "        dataset[ticker] = data;\n",
    "    end\n",
    "end\n",
    "my_list_of_tickers = keys(dataset) |> collect |> x->sort(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91ffd4f-ee73-4f70-9998-fef0af3d596b",
   "metadata": {},
   "source": [
    "The single index model uses a market index, e.g., the [S&P500](https://en.wikipedia.org/wiki/S%26P_500), as the base from which we compute the return for firm $i$. Let's get the index in our dataset that corresponds to [SPDR SPY](https://www.ssga.com/us/en/intermediary/etfs/capabilities/spdr-core-equity-etfs/spy-sp-500), an ETF that tracks the [S&P500](https://en.wikipedia.org/wiki/S%26P_500). \n",
    "* We'll use the [findfirst function](https://docs.julialang.org/en/v1/base/strings/#Base.findfirst-Tuple{AbstractString,%20AbstractString}) on the `my_list_of_tickers` list to find the index that corresponds to `SPY`, we'll use this later. We'll store this value in the `idx_spy` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2575057-a9fe-4498-9842-ccb973f6d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_spy = findfirst(x->x==\"SPY\", my_list_of_tickers); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf13ad-6b44-49f0-b903-6e5ffe5b1d76",
   "metadata": {},
   "source": [
    "Finally, we must calculate the `excess return` from the price information stored in the `dataset.` We'll do this for every firm in the `dataset` using the `μ(...)` function. We'll store the excess return data in the `R` matrix, and the excess return for `SPY` in the `Rₘ` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e73149a-ec13-4ab2-9584-a988dc9b7179",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_free_rate = 0.05;\n",
    "R = μ(dataset, my_list_of_tickers, risk_free_rate = risk_free_rate) |> x-> transpose(x) |> Matrix;\n",
    "Rₘ = R[idx_spy, :]; # this is the growth rate of the market, which we apoproximate by the SPY ETF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a6edff6-92d8-4e1b-805d-ae548d170b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460×1507 Matrix{Float64}:\n",
       "  0.324988   2.13987    0.886321  …  -0.253042    0.82831    -2.05079\n",
       "  1.87928   -0.668139  -1.96617      -2.39417    -0.714768   -3.93842\n",
       "  6.97769    3.97037   -0.853754      0.445564    0.422411   -1.25519\n",
       " -0.229406   2.41956   -0.137144     -0.838392    1.71607    -1.80534\n",
       " -0.441052   2.99641   -3.21336       0.0326048   0.346354   -0.125154\n",
       "  0.696226  -0.486171  -0.477924  …   0.76565     1.80387    -0.60306\n",
       "  2.69226    1.31323    2.27321       0.100489   -1.10553    -0.908103\n",
       "  3.5503     2.67761   -0.262282     -1.00299    -0.113779    0.426391\n",
       "  1.0133     0.69821    0.131699      0.176363    1.96033    -2.42499\n",
       "  3.04632   -0.875146  -1.36581      -0.772932    0.676418   -0.600294\n",
       "  2.59331   -0.863023  -0.31646   …  -0.549034    0.66048     0.551563\n",
       "  5.21212    0.861856  -1.53733       0.728394   -0.132987   -1.3564\n",
       " -1.54435   -1.75771    1.9039       -0.987525    2.13251    -0.334743\n",
       "  ⋮                               ⋱               ⋮          \n",
       "  1.44958   11.9403    17.3661       -0.328214    2.64216     0.218911\n",
       " -0.945092  -0.537242   0.71179       1.18202     1.5781     -1.27262\n",
       "  0.48294    3.98547   -3.45625   …  -1.38846     3.64616    -1.3721\n",
       " -1.55507   -3.32717    0.499164     -3.14093     1.03942    -0.369354\n",
       "  1.67823   -1.3525     1.76427      -1.6492     -2.79552    -2.02999\n",
       " -0.160686   1.7563     2.83286       0.896374   -0.0549493  -0.653836\n",
       "  3.01441   -0.418097   0.651176      1.67148     0.0807654   0.1089\n",
       "  2.14791    1.9062    -0.174217  …   0.8575     -0.338764   -0.0318702\n",
       " -0.98186    1.85109    1.36242      -0.148925    0.92446    -0.027224\n",
       "  5.75159    3.95668    2.84928       1.24036    -0.216922   -1.21052\n",
       "  1.74882   -0.761936  -0.405978     -0.947707   -0.243954   -2.9499\n",
       "  2.00452    2.9259     2.41921       1.18925     1.00671     0.0621309"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31607a29-5f4b-4623-b407-43d489a3c688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1507-element Vector{Float64}:\n",
       "  1.5050377454909538\n",
       "  1.095468088086578\n",
       "  0.7757592626051124\n",
       "  0.7390204094703445\n",
       " -0.659217123297486\n",
       "  1.2784386070568685\n",
       "  1.8262837004768115\n",
       "  0.5011457033442471\n",
       "  0.5948732710078719\n",
       "  0.44603924309987253\n",
       "  0.4564092862912787\n",
       "  1.7522322189385078\n",
       "  1.0553224381234976\n",
       "  ⋮\n",
       "  2.4657453266301013\n",
       "  2.1767892155556128\n",
       " -1.3150088568715164\n",
       "  1.292192524780104\n",
       "  1.0803119750549104\n",
       " -1.2770490306414208\n",
       " -0.28967894950228634\n",
       "  1.3223918641120702\n",
       "  0.6465841382666371\n",
       "  0.299345721134233\n",
       "  0.4813841702141785\n",
       " -0.7968919650587696"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rₘ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac319c7-3d6a-4561-b4c8-f1e6e4195e0e",
   "metadata": {},
   "source": [
    "## Task 1: Compute $\\theta$ using pseudo-inverse and SVD approaches\n",
    "Regularized least squares estimates of the unknown parameters $\\mathbf{\\beta}$ that minimizes the sum of squared errors between the model calculated and observed outputs plus a penalty term:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{\\theta}}_{\\lambda} = \\arg\\min_{\\mathbf{\\theta}} ||~\\mathbf{y} - \\mathbf{X}\\cdot\\mathbf{\\theta}~||^{2}_{2} + \\lambda\\cdot||~\\mathbf{\\theta}~||^{2}_{2}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $||\\star||^{2}_{2}$ is the square of the p = 2 vector norm, $\\lambda\\geq{0}$ denotes the regularization parameter, and $\\hat{\\mathbf{\\theta}}$ denotes the estimated parameter vector. \n",
    "The $\\hat{\\mathbf{\\theta}}$ that minimizes the $||\\star||^{2}_{2}$ loss plus penalty for data matrix $\\mathbf{X}$ is given by:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{\\theta}}_{\\lambda} = \\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\cdot\\mathbf{I}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{y} - \\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\cdot\\mathbf{I}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{\\epsilon}\n",
    "\\end{equation*}\n",
    "$$\n",
    "The matrix $\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\cdot\\mathbf{I}$ is called the _regularized normal matrix_, while $\\mathbf{X}^{T}\\mathbf{y}$ is called the _moment vector_. The matrix $\\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\cdot\\mathbf{I}\\right)^{-1}$ must exist for the solution $\\hat{\\mathbf{\\theta}}_{\\lambda}$ to exist.\n",
    "\n",
    "#### SVD approach\n",
    "Alternatively, we can compute the parameters using  [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition). Let the [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) of the $n\\times{p}$ data matrix $\\mathbf{X}$ be given by:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\mathbf{U}\\cdot\\mathbf{\\Sigma}\\cdot\\mathbf{V}^{T}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\mathbf{U}$ is an orthogonal matrix, $\\mathbf{\\Sigma}$ is a diagonal singular value matrix,\n",
    "and $\\mathbf{V}$ is an orthogonal matrix. Then, the regularized least-squares estimate of the unknown parameter vector $\\mathbf{\\theta}$ is given by:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{\\theta}}_{\\lambda} = \\Bigl[\\left(\\mathbf{\\Sigma}^{T}\\mathbf{\\Sigma}+\\lambda\\mathbf{I}\\right)\\mathbf{V}^{T}\\Bigr]^{-1}\\cdot\\mathbf{\\Sigma^{T}}\\cdot\\mathbf{U}^{T}\\cdot\\mathbf{y}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eaf4994-8ca7-48a8-8c0a-07833c89e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 100.0;\n",
    "ticker_to_explore = \"IBM\";\n",
    "idx_of_ticker = findfirst(x->x==ticker_to_explore, my_list_of_tickers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77b13732-df38-4b2a-88cc-a1d96cb98ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -0.0791262235703992\n",
       "  0.8810978201605225"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "θ̂_pinv = θ(R,idx_of_ticker, Rₘ, λ = λ, method = MyMatrixAlgebraLearningMethod())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b86ea965-62bc-4552-ba24-58735c3f914d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -0.07912622357039918\n",
       "  0.8810978201605225"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "θ̂_svd = θ(R,idx_of_ticker, Rₘ, λ = λ, method = MySVDLearingMethod())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae09650-cef5-4e2a-8205-e75a74df9483",
   "metadata": {},
   "source": [
    "#### Check: Do the two approaches give the same answer?\n",
    "Let's use the [@assert macro](https://docs.julialang.org/en/v1/base/base/#Base.@assert) to check if the parameter vectors `θ̂_pinv` and `θ̂_svd` are `close`. If this test fails, [an AssertionError](https://docs.julialang.org/en/v1/base/base/#Core.AssertionError) is thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2c5d14b-0c65-48fb-b958-f9876796d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert θ̂_pinv ≈ θ̂_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acfbada-744b-43b6-8194-7782bf89b655",
   "metadata": {},
   "source": [
    "## Task 2: A deeper look at the SVD approach\n",
    "One of the exciting things about using the [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) to solve the linear regression problem for the unknown parameter vector $\\hat{\\theta}_{\\lambda}$ is the summation relationship:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\hat{\\theta}_{\\lambda} = \\sum_{i=1}^{r}\\frac{\\sigma_{i}(\\mathbf{u}_{i}^{T}\\cdot\\mathbf{y})}{\\sigma_{i}^{2}+\\lambda}\\cdot\\mathbf{v}_{i}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $r$ denotes the rank of the data matrix $\\mathbf{X}$, $\\mathbf{u}_{i}$ and $\\mathbf{v}_{i}$ are the $i$-th columns of $\\mathbf{U}$ and $\\mathbf{V}$, respectively, $\\sigma_{i}$ is the $i$-th singular value, and $\\lambda$ is the regularization parameter.\n",
    "This tells us what each mode of the [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) contributes to the unknown parameter vector $\\hat{\\theta}_{\\lambda}$, which is __super cool__!!\n",
    "* Let's reconstruct the unknown parameters by summing the `r` modes of the data matrix $\\mathbf{X}$. First, let's set up the data matrix $\\mathbf{X}$, and compute the [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) using the [svd function](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.svd) exported by the [Linear Algebra package](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#man-linalg) in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "182acd00-db9c-42a2-a4f3-da4c4db64d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = R[idx_of_ticker, :];\n",
    "max_length = length(Y);\n",
    "X = [ones(max_length) Rₘ];\n",
    "IM = diagm(ones(2)); # we have two parameters -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb92d694-0ce1-4ec8-827a-71a65aca4107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1507×2 Matrix{Float64}:\n",
       " 1.0   1.50504\n",
       " 1.0   1.09547\n",
       " 1.0   0.775759\n",
       " 1.0   0.73902\n",
       " 1.0  -0.659217\n",
       " 1.0   1.27844\n",
       " 1.0   1.82628\n",
       " 1.0   0.501146\n",
       " 1.0   0.594873\n",
       " 1.0   0.446039\n",
       " 1.0   0.456409\n",
       " 1.0   1.75223\n",
       " 1.0   1.05532\n",
       " ⋮    \n",
       " 1.0   2.46575\n",
       " 1.0   2.17679\n",
       " 1.0  -1.31501\n",
       " 1.0   1.29219\n",
       " 1.0   1.08031\n",
       " 1.0  -1.27705\n",
       " 1.0  -0.289679\n",
       " 1.0   1.32239\n",
       " 1.0   0.646584\n",
       " 1.0   0.299346\n",
       " 1.0   0.481384\n",
       " 1.0  -0.796892"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baf9db3-fde5-4b5d-adac-5a5ceeb9909c",
   "metadata": {},
   "source": [
    "Compute the [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) using the [svd function](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.svd) exported by the [Linear Algebra package](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#man-linalg) in Julia. By default, the Julia [svd function](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.svd) returns the singular values as a vector, so let's use the [diagm function](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.diagm) to compute a diagonal matrix from the singular value vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bcfb5c7-1b85-4bd2-bfd1-35ab6d005ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Float64}:\n",
       " 97.874   0.0\n",
       "  0.0    38.8129"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(U,d,V) = svd(X);\n",
    "Σ = diagm(d) # the Julia SVD returns only the diagonal vector, using diagm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a12aa-9425-46aa-b40a-ddcbc343c4b8",
   "metadata": {},
   "source": [
    "Next, let's compute the [rank](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.rank) of the data matrix $\\mathbf{X}\\in\\mathbb{R}^{n\\times{p}}$. We know that the rank $r\\leq\\min\\left(n,p\\right)$. But what is our intuition about rank?\n",
    "* __Alternative view__: The [rank](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.rank) is also the number of non-zero singular values; thus, we can think of the [rank](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.rank) as the number of unique pieces of information that are contained in the data matrix $\\mathbf{X}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1edb4d2b-977e-474e-a4d1-b8d217117048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = rank(X) # the number of non-zero modes in the svd decomposition!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedfce73-05aa-4454-8f4a-925d4b12167d",
   "metadata": {},
   "source": [
    "Now, we can compute the contribution to the unknown parameter vector $\\theta$ that comes from each mode of the [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition). The most important mode corresponds to the $\\beta$ parameter (risk), followed by the $\\alpha$, which is a measure of the firm-specific excess return:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "162d1c8c-2ca1-47f4-a2db-10a008539099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i, θ̂) = (1, [0.007332794297039036, 0.8803776903797778])\n",
      "(i, θ̂) = (2, [-0.0791262235703991, 0.8810978201605227])\n"
     ]
    }
   ],
   "source": [
    "θ̂ = zeros(2); # we have two parameters -\n",
    "for i ∈ 1:r\n",
    "    \n",
    "\n",
    "    σᵢ = Σ[i,i];\n",
    "    uᵢ = U[:,i];\n",
    "    vᵢ = V[:,i];\n",
    "\n",
    "    # compute the coefficient -\n",
    "    cᵢ = (σᵢ*dot(transpose(uᵢ),Y))/(σᵢ^2 + λ)\n",
    "\n",
    "    # compute the parameter update -\n",
    "    θ̂ += cᵢ*vᵢ\n",
    "\n",
    "    @show (i, θ̂)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.0",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
