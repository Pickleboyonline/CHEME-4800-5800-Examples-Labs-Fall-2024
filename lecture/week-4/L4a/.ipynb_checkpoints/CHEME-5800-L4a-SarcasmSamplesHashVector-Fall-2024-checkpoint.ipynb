{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f298a06-6f24-4090-8778-0eadd4729806",
   "metadata": {},
   "source": [
    "# Example: Feature Hashing of Sarcasm Samples\n",
    "Another strategy to represent text in mathematical form is to [use a feature-hashing approach](https://en.wikipedia.org/wiki/Feature_hashing) that takes the text and projects it into high-dimensional vectors (called feature vectors) that live in the space of the token dictionary. Thus, each text blurb, e.g., a headline, can represented as a vector in this space. This approach handles the padding challenges we saw previously but has some of its own issues.\n",
    "\n",
    "### Learning objectives\n",
    "This example will familiarize students with using [the Weinberger feature hashing algorithm](https://en.wikipedia.org/wiki/Feature_hashing) to compute high-dimensional vectors representing unstructured text. This approach differs from the tokenizer vectors we created in the previous example.\n",
    "* __Prerequisites__: To save some time, we'll load the saved file from the `SarcasmSamplesTokenizer` example using [the `load(...)` method exported by the FileIO.jl package](https://github.com/JuliaIO/FileIO.jl). We can then pull out some stuff we computed last time and reuse it here.\n",
    "* __Task 1__: Compute the feature hash vectors for the sarcastic samples. In this task, we'll compute [the feature hash vector representation](https://en.wikipedia.org/wiki/Feature_hashing) of the text headline for each sarcastic sample.\n",
    "* __Task 2__: Compute the feature hash vectors for the unsarcastic samples. In this task, we'll do a similar computation as task 1, except we'll construct the feature vectors for the non-sarcastic headline samples.\n",
    "* __Task 3__: Can we compute a value for the similarity of the feature vectors? In this task, we calculate the similarity of feature vectors using [a kernel function](https://en.wikipedia.org/wiki/Kernel_method)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9500dca4-1aa5-4b35-ad3f-942212a96a32",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We set up the computational environment by including [the `Include. jl` file](Include.jl) using [the `include(...)` method](https://docs.julialang.org/en/v1/base/base/#Base.include). The [`Include.jl` file](Include.jl) loads external packages and functions we will use in these examples. \n",
    "* For additional information on functions and types used in this example, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ab154de-1a83-4a53-a419-7a9adfe8c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c1986b-3c5c-4780-a167-9ac1608f9a63",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "To save some time, we'll load the saved file from the `SarcasmSamplesTokenizer` example using [the `load(...)` method exported by the FileIO.jl package](https://github.com/JuliaIO/FileIO.jl). To load the `jld2` (binary) saved file, we pass the path to the file we want to load the [`load(...)` function](https://github.com/JuliaIO/FileIO.jl). This call returns the data as a [Julia `Dict` type](https://docs.julialang.org/en/v1/base/collections/#Base.Dict). \n",
    "* Let's set the path to the save file in the `path_to_save_file::String` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50411853-dbc0-4176-9816-17e519445864",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_file = joinpath(_PATH_TO_DATA, \"L4a-SarcasmSamplesTokenizer-SavedData.jld2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e89e16-7534-4128-a532-d94c81bbcd64",
   "metadata": {},
   "source": [
    "Then we load the `jld2` file using [the `load(...)` method](https://juliaio.github.io/FileIO.jl/stable/reference/#FileIO.load), where the contents of the file are stored in the `saved_data_dictionary::Dict{String, Any}` variable. \n",
    "* We saved the `corpusmodel::MySarcasmRecordCorpusModel` instance, which holds the other interesting data, e.g., the `tokendictionary.` Thus, we can get (most) of everything we need from the `corpusmodel.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74be06da-ef39-4af5-93b2-bc70587e58f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data_dictionary = load(path_to_save_file);\n",
    "\n",
    "# pull data from the saved_data_dictionary -\n",
    "corpusmodel = saved_data_dictionary[\"corpus\"];\n",
    "\n",
    "tokendictionary = corpusmodel.tokens;\n",
    "inversetokendictionary = corpusmodel.inverse;\n",
    "number_of_records = saved_data_dictionary[\"number_of_records\"];\n",
    "\n",
    "# compute some stuff need for later -\n",
    "number_of_tokens = tokendictionary |> length; # size of the token dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9fdda5a-8a7a-4273-9770-8ae8171faefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 2 entries:\n",
       "  \"corpus\"            => MySarcasmRecordCorpusModel(Dict{Int64, MySarcasmRecord…\n",
       "  \"number_of_records\" => 28619"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_data_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fded10a7-2d73-485e-bb47-40954aab8952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySarcasmRecordModel(true, \"thirtysomething scientists unveil doomsday clock of hair loss\", \"https://www.theonion.com/thirtysomething-scientists-unveil-doomsday-clock-of-hai-1819586205\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusmodel.records[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd2257-cbe3-4b5e-98a7-9c1853bd9e90",
   "metadata": {},
   "source": [
    "## Task 1: Compute the feature hash vectors for sarcastic samples\n",
    "In this task, we'll compute [the feature hash vector representation](https://en.wikipedia.org/wiki/Feature_hashing) of the text headline for each sarcastic sample using the token dictionary we developed in the previous example. We'll save these feature vectors in the `sarcasim_hashed_dictionary::Dict{Int64, Vector{Int64}}` variable, where the keys are the sample indexes, and the values are the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f6fa3b9-f464-420a-a452-b2813fbbdf80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64, Vector{Int64}} with 13634 entries:\n",
       "  11950 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
       "  1703  => [0, 0, 0, 0, 2, 0, 0, 0, 0, 0  …  0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  18374 => [0, 0, 0, 1, 1, 0, 1, 0, 0, 1  …  0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
       "  23970 => [0, 0, 0, 0, 0, 1, 0, 0, 0, 0  …  0, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
       "  27640 => [0, 0, 0, 1, 0, 0, 0, 1, 2, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
       "  28576 => [0, 0, 0, 0, 1, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
       "  2015  => [0, 0, 0, 0, 0, 1, 0, 0, 0, 0  …  0, 0, 0, 1, 0, 0, 0, 0, 2, 0]\n",
       "  11280 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
       "  28165 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  3220  => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 1, 0, 1]\n",
       "  422   => [1, 0, 0, 0, 0, 1, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  15370 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  15859 => [0, 0, 0, 0, 0, 0, 0, 1, 0, 0  …  0, 1, 1, 0, 1, 0, 0, 0, 0, 0]\n",
       "  4030  => [0, 0, 0, 0, 0, 0, 0, 0, 0, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  3163  => [0, 0, 0, 0, 0, 0, 0, 0, 0, 1  …  0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
       "  9523  => [0, 0, 0, 0, 0, 0, 1, 0, 0, 0  …  0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
       "  25334 => [0, 1, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
       "  25581 => [0, 0, 0, 0, 0, 0, 0, 0, 2, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  16341 => [0, 0, 0, 0, 0, 0, 0, 0, 1, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  23265 => [0, 0, 0, 1, 0, 1, 0, 0, 1, 0  …  0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
       "  12797 => [0, 0, 0, 2, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
       "  14085 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
       "  27851 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
       "  10454 => [0, 0, 1, 0, 1, 0, 0, 0, 0, 0  …  0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
       "  24477 => [1, 0, 0, 0, 1, 0, 0, 0, 0, 0  …  1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
       "  ⋮     => ⋮"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasim_hashed_dictionary = let\n",
    "\n",
    "    length_of_feature_vector = length(tokendictionary) + 1;\n",
    "    #length_of_feature_vector = 64;\n",
    "    \n",
    "    sarcasim_hashed_dictionary = Dict{Int64, Array{Int64,1}}();\n",
    "    for i ∈ 1:number_of_records\n",
    "        record = corpusmodel.records[i];\n",
    "        is_sarcastic_flag = record.issarcastic\n",
    "        if (is_sarcastic_flag == true)        \n",
    "            fields = split(record.headline, ' ') .|> String        \n",
    "            sarcasim_hashed_dictionary[i] = hashing(fields, hash = tokendictionary, \n",
    "                size = length_of_feature_vector);\n",
    "        end\n",
    "    end\n",
    "    sarcasim_hashed_dictionary\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46f6db-1291-4f78-a3c4-05f4d8b028a5",
   "metadata": {},
   "source": [
    "In this representation, most of the values in the hashed dictionary are zeros. However, the non-zero entries correspond to the index of the word in the token dictionary, where the value at that position is the count of the words in the sample. \n",
    "\n",
    "For example, the headline at index `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86f8362e-0139-4ac6-9f85-32a6131facb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thirtysomething scientists unveil doomsday clock of hair loss\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusmodel.records[1].headline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3bf177-7739-4407-9cbf-f32444dd466d",
   "metadata": {},
   "source": [
    "has the following non-zero entries in the hashed dictionary, which we calculate using [the `findall(...)` method](https://docs.julialang.org/en/v1/base/arrays/#Base.findall-Tuple{Any}). \n",
    "* The first argument to the [the `findall(...)` method](https://docs.julialang.org/en/v1/base/arrays/#Base.findall-Tuple{Any}) is an example of [an anonymous functions](https://docs.julialang.org/en/v1/manual/functions/#man-anonymous-functions). It evaluates to a boolean condition for each value passed in. Thus, the call to [the `findall(...)` method](https://docs.julialang.org/en/v1/base/arrays/#Base.findall-Tuple{Any}) returns the indexes of the values that meet the condition `x != 0`\n",
    "\n",
    "The non-zero indexes correspond to the indexed of the words in the `tokendictionary::Dict{String,Int64}`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6589e3a-c843-4f88-9fe8-d037a68ca121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{Int64}:\n",
       " 13\n",
       " 16\n",
       " 21\n",
       " 38\n",
       " 40\n",
       " 50\n",
       " 58\n",
       " 64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = findall(x -> x != 0, sarcasim_hashed_dictionary[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff109438-e5d2-4836-907b-5f535bd11b75",
   "metadata": {},
   "source": [
    "where each word occurs only once in this particular headline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69951ebe-20c6-46c9-9641-5f3a3cb49735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{Int64}:\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasim_hashed_dictionary[1][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5e8b1dd-024e-4977-81e0-4692a71b6806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hair\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inversetokendictionary[12048 - 1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb79c96-b3f8-4431-aed3-ea2c58d05657",
   "metadata": {},
   "source": [
    "### Check: What if we have a repeated word? \n",
    "Let's quickly check an example where words, i.e., tokens, are repeated in the headline.  For example, let's consider the headline at index `28616`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff440b15-acc2-428c-af2e-2c8fbe42022b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"internal affairs investigator disappointed conspiracy doesnt go all the way to the top\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusmodel.records[28616].headline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71390c06-baca-4dce-8b6a-f81eac94b48f",
   "metadata": {},
   "source": [
    "The `the` token is repeated in the sentence. Thus, we should have a value of `2` in the hashed feature vector at the position corresponding to the token `the.` \n",
    "* Let's check this out; first, find the indexes of the non-zero elements of the feature vector, then look at the values in the feature vector at those indexes, and finally, show that the index holding a value of `2` corresponds to the `the` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "139a2546-de4b-4214-bed8-4ad33f13e0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j = [1, 1, 1, 1, 1, 3, 3, 1, 1]\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "AssertionError: tokendictionary[\"the\"] == i[k] - 1",
     "output_type": "error",
     "traceback": [
      "AssertionError: tokendictionary[\"the\"] == i[k] - 1",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[75]:13"
     ]
    }
   ],
   "source": [
    "let\n",
    "    story_index_to_check = 28616; # What story do we want to look at?\n",
    "    i = findall(x -> x != 0, sarcasim_hashed_dictionary[story_index_to_check]); # find indexes non-zero fv values\n",
    "\n",
    "    j = sarcasim_hashed_dictionary[story_index_to_check][i]; # elements of the feature vector\n",
    "    \n",
    "    # Find the index corresponding to `2` in the feature vector\n",
    "    k = findfirst(x -> x != 1, j) # Note: this the index of the i-vector\n",
    "\n",
    "    # check: if not equal to `the`, we'll get an AssertionError\n",
    "    @assert tokendictionary[\"the\"] == (i[k] - 1) # why -1?\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc9520-4a1f-4ad3-a767-0df83d890d7e",
   "metadata": {},
   "source": [
    "### Check: Can we recreate the headline from the hashed feature vector?\n",
    "Let's quickly check to see if we can recreate the original headline using the feature vector representation of the text. For example, can we reassemble the headline at index `1` in the `sarcasim_hashed_dictionary`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "140ebeeb-c551-49c3-87c8-01562637ad7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"#brownribboncampaign #digitalhealth #feelthebern #napaquake #nevertrump #starwarschristmascarols #trickortreatin100years #xmasgiftsfromtrump\", \"thirtysomething scientists unveil doomsday clock of hair loss\")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let\n",
    "    h = corpusmodel.records[1].headline; # headline string from the corpusmodel\n",
    "    fv = sarcasim_hashed_dictionary[1]; # get the feature vector from the sarcasim_hashed_dictionary, save this in fv\n",
    "    iv = findall(x -> x != 0, fv) # find indexes non-zero fv values\n",
    "    \n",
    "    tmp = Array{String,1}();\n",
    "    for i ∈ iv\n",
    "        push!(tmp, inversetokendictionary[i-1]); # inverse is 0-based\n",
    "        push!(tmp,\" \");\n",
    "    end\n",
    "\n",
    "    test_headline = join(tmp) |> strip # same as tmp |> join |> strip. recreate the headline string, strip off the trailing space\n",
    "    # @assert h == test_headline # are the same?\n",
    "    test_headline, h\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eb5dd7-03ee-4a9e-85c8-d86ce9376dcd",
   "metadata": {},
   "source": [
    "__Hmmm__: That didn't go like expected. What happened??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab786f8c-83e7-451b-a23c-76b2e4c8d6a1",
   "metadata": {},
   "source": [
    "## Task 2: Compute the feature hash vectors for the unsarcastic samples\n",
    "In this task, we'll do a similar computation as task 1, except we'll construct the feature vectors for the non-sarcastic headline samples. We'll save these in the `unsarcasim_hashed_dictionary::Dict{Int64, Array{Int64,1}}` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c78d7d40-7793-49f0-b44a-fc500e25fcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64, Vector{Int64}} with 14985 entries:\n",
       "  12427 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  7685  => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  3406  => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  1090  => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  18139 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  17088 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  16805 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  11251 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  25327 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  8060  => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  14167 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  8660  => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  18475 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  14221 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  15545 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  3855  => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  15916 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  23629 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  22241 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  2989  => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  4357  => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  17494 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  8552  => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  9266  => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  23690 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "  ⋮     => ⋮"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsarcasim_hashed_dictionary = let         \n",
    "    unsarcasim_hashed_dictionary = Dict{Int64, Array{Int64,1}}();\n",
    "    for i ∈ 1:number_of_records\n",
    "        record = corpusmodel.records[i];\n",
    "        is_sarcastic_flag = record.issarcastic\n",
    "        if (is_sarcastic_flag == false)\n",
    "            \n",
    "            # split -\n",
    "            fields = split(record.headline, ' ') .|> String        \n",
    "            unsarcasim_hashed_dictionary[i] = hashing(fields, \n",
    "                hash = tokendictionary, size = (number_of_tokens+1));\n",
    "        end\n",
    "    end\n",
    "    unsarcasim_hashed_dictionary\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c856aa-7dd2-4bcd-aafb-bc03530e3eff",
   "metadata": {},
   "source": [
    "## Task 3: Can we compute a value for the similarity of the feature vectors?\n",
    "In this task, we calculate the similarity of feature vectors using [a kernel function](https://en.wikipedia.org/wiki/Kernel_method). A [kernel function](https://en.wikipedia.org/wiki/Kernel_method) is a measure of the similarity between two feature vectors $\\mathbf{x}\\in\\mathcal{X}$ and $\\mathbf{x}^{\\prime}\\in\\mathcal{X}$ such that $k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}$. In this task, we'll use the squared exponential kernel function defined as:\n",
    "$$\n",
    "\\begin{equation}\n",
    "k\\left(\\mathbf{x},\\mathbf{x}^{\\prime}\\right) = \\exp\\left(-\\gamma\\cdot{d\\left(\\mathbf{x},\\mathbf{x}^{\\prime}\\right)^{2}}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "where $d\\left(\\mathbf{x},\\mathbf{x}^{\\prime}\\right) = ||\\mathbf{x} - \\mathbf{x}^{\\prime}||_{2}$, i.e., the $L^{2}$-norm of the difference between $\\mathbf{x}$ and $\\mathbf{x}^{\\prime}$, and $\\gamma$ is an adjustable length scale parameter.\n",
    "* if $d\\left(\\mathbf{x},\\mathbf{x}^{\\prime}\\right)\\rightarrow\\infty$, the kernel function $k\\left(\\mathbf{x},\\mathbf{x}^{\\prime}\\right)\\rightarrow{0}$, i.e., for a large distance between the feature vectors, we'll have a small similarity score.\n",
    "* if $d\\left(\\mathbf{x},\\mathbf{x}^{\\prime}\\right)\\rightarrow{0}$, the kernel function $k\\left(\\mathbf{x},\\mathbf{x}^{\\prime}\\right)\\rightarrow{1}$, i.e., for a small distance between the feature vectors, we'll have a large similarity score, in this case a value of `1.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902c99b-4371-4fe6-a22c-40bed3afad19",
   "metadata": {},
   "source": [
    "### Similarity between sarcastic samples\n",
    "Let's use the [kernel function](https://en.wikipedia.org/wiki/Kernel_method) to compute the similarity between the like samples, e.g., different sarcasm samples. We'll use the [the `SqExponentialKernel()` function exported from the KernelFunctions.jl package](https://github.com/JuliaGaussianProcesses/KernelFunctions.jl/tree/master).\n",
    "* What is going with [the `with_lengthscale(...)` function](https://juliagaussianprocesses.github.io/KernelFunctions.jl/stable/userguide/#Kernel-Creation)? We set the $\\gamma$ parameter, i.e., the lengthscale or the gain of the squared distance, using this function. We save the kernel function in the `k` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a24ab2bb-76d9-4032-bdc3-0524e4e382f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = with_lengthscale(SqExponentialKernel(), 2.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a579a77-162d-4a1f-a6ce-92aea9693915",
   "metadata": {},
   "source": [
    "Given our kernel (similarity) function, we can iterate through the sarcasm samples and compute a `sarcasim_distance_matrix::Array{Float16,2}` which is an $N\\times{N}$ lower triangular matrix with the self-similarity values on the diagonal, and the comparison similararities in the off-diagonal positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "302e0aa2-f2a2-48d6-867d-4bb3acdeda5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcasim_distance_matrix = let\n",
    "\n",
    "    number_of_sarchastic_samples = 20; # let's look at the first N\n",
    "    samples = keys(sarcasim_hashed_dictionary) |> collect |> sort; # get a sorted list of keys\n",
    "    sarcasim_distance_matrix = Array{Float16,2}(undef, number_of_sarchastic_samples, number_of_sarchastic_samples) |> x->fill!(x,0.0);\n",
    "\n",
    "    # process N keys -\n",
    "    for i ∈ 1:number_of_sarchastic_samples\n",
    "        x = sarcasim_hashed_dictionary[samples[i]];\n",
    "        \n",
    "        for j ∈ 1:i\n",
    "            x′ = sarcasim_hashed_dictionary[samples[j]]\n",
    "            sarcasim_distance_matrix[i,j] = k(x,x′) |> Float16\n",
    "        end\n",
    "    end\n",
    "    sarcasim_distance_matrix\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e064b96e-0250-498d-aa44-049df86bfa96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20×20 Matrix{Float16}:\n",
       " 1.0      0.0      0.0      0.0      0.0      …  0.0     0.0     0.0     0.0\n",
       " 0.1354   1.0      0.0      0.0      0.0         0.0     0.0     0.0     0.0\n",
       " 0.11945  0.1969   1.0      0.0      0.0         0.0     0.0     0.0     0.0\n",
       " 0.11945  0.04395  0.0821   1.0      0.0         0.0     0.0     0.0     0.0\n",
       " 0.1054   0.1738   0.093    0.05643  1.0         0.0     0.0     0.0     0.0\n",
       " 0.1533   0.07245  0.1054   0.0388   0.05643  …  0.0     0.0     0.0     0.0\n",
       " 0.1533   0.11945  0.1354   0.0821   0.1969      0.0     0.0     0.0     0.0\n",
       " 0.253    0.1533   0.1738   0.0821   0.1969      0.0     0.0     0.0     0.0\n",
       " 0.0821   0.1354   0.093    0.0342   0.0639      0.0     0.0     0.0     0.0\n",
       " 0.0821   0.04977  0.07245  0.0342   0.1354      0.0     0.0     0.0     0.0\n",
       " 0.0388   0.1054   0.093    0.01616  0.04977  …  0.0     0.0     0.0     0.0\n",
       " 0.1354   0.1738   0.11945  0.05643  0.0821      0.0     0.0     0.0     0.0\n",
       " 0.0639   0.0388   0.05643  0.04395  0.02351     0.0     0.0     0.0     0.0\n",
       " 0.253    0.11945  0.1354   0.1054   0.093       0.0     0.0     0.0     0.0\n",
       " 0.093    0.05643  0.0821   0.0821   0.04395     0.0     0.0     0.0     0.0\n",
       " 0.1738   0.0639   0.11945  0.05643  0.0639   …  0.0     0.0     0.0     0.0\n",
       " 0.1354   0.2866   0.1533   0.05643  0.2231      1.0     0.0     0.0     0.0\n",
       " 0.1054   0.1354   0.1969   0.07245  0.1354      0.2866  1.0     0.0     0.0\n",
       " 0.1054   0.0821   0.1533   0.05643  0.1054      0.1054  0.1738  1.0     0.0\n",
       " 0.368    0.1738   0.1969   0.1533   0.1354      0.2231  0.1738  0.1738  1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasim_distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcb8bcd-b99c-4a6c-8229-705eeec06074",
   "metadata": {},
   "source": [
    "Let's look at similar headlines with significant similarity scores, e.g., index `20` and `1`, which have a similarity score of `0.2231`, versus non-similar ones, such as `13` and `1`, which have a similarity score of `0.0388.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eac2a23e-3035-4a98-97ed-5c927267d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_sarchastic = keys(sarcasim_hashed_dictionary) |> collect |> sort;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90150a-b331-4339-b2c7-42e1ba68440a",
   "metadata": {},
   "source": [
    "#### Similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7aa051d3-5cc4-48b9-b69e-431e1dd63751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySarcasmRecordModel(true, \"report make it stop\", \"https://www.theonion.com/report-make-it-stop-1822874962\")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusmodel.records[samples_sarchastic[20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76dc4307-b44d-4b4f-a17e-2ff42a7ccbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySarcasmRecordModel(true, \"thirtysomething scientists unveil doomsday clock of hair loss\", \"https://www.theonion.com/thirtysomething-scientists-unveil-doomsday-clock-of-hai-1819586205\")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusmodel.records[samples_sarchastic[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0842ef-25e3-41d7-8ed4-3b0a9283dc75",
   "metadata": {},
   "source": [
    "#### Not similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40437da0-0415-4b87-abec-05d58654953d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySarcasmRecordModel(true, \"expansive obama state of the union speech to touch on patent law entomology the films of robert altman\", \"https://politics.theonion.com/expansive-obama-state-of-the-union-speech-to-touch-on-p-1819574546\")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusmodel.records[samples_sarchastic[13]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b909d635-1cc6-4506-999e-ddf69894a175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySarcasmRecordModel(true, \"thirtysomething scientists unveil doomsday clock of hair loss\", \"https://www.theonion.com/thirtysomething-scientists-unveil-doomsday-clock-of-hai-1819586205\")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusmodel.records[samples_sarchastic[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fcf3f2-ae27-4c12-96bb-bd6fa86f32af",
   "metadata": {},
   "source": [
    "### Similarity between sarcastic and unsarcastic samples\n",
    "Ultimately, we want to classify an unseen headline as either sarcastic or not sarcastic. Could we do that using the kernel similarity scores?\n",
    "* `Hypothesis`: Same-type headlines, i.e., `i-i` headlines, will have higher similarity scores than `i-j` combinations where $i\\neq{j}$ and `j` are non-sarcastic headlines. Thus, we would expect cross-similarity scores to have small magnitudes compared with the `i-i` comparisons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "770f74c8-90f6-4bd0-9b9c-eb7adf295c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_not_sarchastic = keys(unsarcasim_hashed_dictionary) |> collect |> sort;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2309a0e-bd9c-4859-bfab-e3cfe2e26859",
   "metadata": {},
   "source": [
    "Let's compute an $N\\times{N}$ cross-similarity matrix where each entry holds the similarity between story `i` (a sarcastic headline) and headline `j` (a non-sarchastic) headline. We'll save this data in the `cross_comparision_matrix::Array{Float16,2}` matrix. Rows will be sarcastic headlines, and the columns will be non-sarcastic headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e1ca497-960e-4f76-8c26-7a1be60a53fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "DimensionMismatch: first array has length 64 which does not match the length of the second, 29665.",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch: first array has length 64 which does not match the length of the second, 29665.",
      "",
      "Stacktrace:",
      " [1] _evaluate",
      "   @ ~/.julia/packages/Distances/n9q0L/src/metrics.jl:244 [inlined]",
      " [2] SqEuclidean",
      "   @ ~/.julia/packages/Distances/n9q0L/src/metrics.jl:328 [inlined]",
      " [3] evaluate(dist::Distances.SqEuclidean, a::Vector{Int64}, b::Vector{Int64})",
      "   @ Distances ~/.julia/packages/Distances/n9q0L/src/generic.jl:24",
      " [4] _scale(t::ScaleTransform{Float64}, metric::Distances.SqEuclidean, x::Vector{Int64}, y::Vector{Int64})",
      "   @ KernelFunctions ~/.julia/packages/KernelFunctions/nkiEB/src/kernels/transformedkernel.jl:34",
      " [5] (::TransformedKernel{SqExponentialKernel{Distances.Euclidean}, ScaleTransform{Float64}})(x::Vector{Int64}, y::Vector{Int64})",
      "   @ KernelFunctions ~/.julia/packages/KernelFunctions/nkiEB/src/kernels/transformedkernel.jl:27",
      " [6] top-level scope",
      "   @ In[45]:12"
     ]
    }
   ],
   "source": [
    "cross_comparision_matrix = let\n",
    "\n",
    "    number_of_cross_samples = 20; # let's look at the first 20\n",
    "    cross_comparision_matrix = Array{Float16,2}(undef, number_of_cross_samples, number_of_cross_samples) |> x->fill!(x,0.0);\n",
    "    \n",
    "    for i ∈ 1:number_of_cross_samples\n",
    "\n",
    "        x = sarcasim_hashed_dictionary[samples_sarchastic[i]];\n",
    "        \n",
    "        for j ∈ 1:number_of_cross_samples\n",
    "            x′ = unsarcasim_hashed_dictionary[samples_not_sarchastic[j]]\n",
    "            cross_comparision_matrix[i,j] = k(x,x′) |> Float16\n",
    "        end\n",
    "    end\n",
    "    cross_comparision_matrix\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34bdb0-ddd7-4567-b9cb-2cf0ba10995c",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "* Could we use the headline hashed feature vectors and the similarity scores computed using the squared exponential kernel function to classify sarcasm versus non-sarcasm? (__Hint__: look at some of the elements of the cross similarity matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
